{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets\n",
    "\n",
    "Some of the most interesting studies of data come from combining different data sources.\n",
    "These operations can involve anything from very straightforward concatenation of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets.\n",
    "``Series`` and ``DataFrame``s are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward.\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Concat and Append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here we'll take a look at simple concatenation of ``Series`` and ``DataFrame``s with the ``pd.concat`` function; later we'll dive into more sophisticated in-memory merges and joins implemented in Pandas.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll define this function which creates a ``DataFrame`` of a particular form that will be useful below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# example DataFrame\n",
    "make_df('ABC', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we'll create a quick class that allows us to display multiple ``DataFrame``s side by side. The code makes use of the special ``_repr_html_`` method, which IPython uses to implement its rich object display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to understand or study this function.  The use of this function is to make it easier to understand how the operations work on multiple dataframes.  It will become clearer as we continue our discussion in the following section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Concatenation with ``pd.concat``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a function, ``pd.concat()`` that contains a number of options that we'll discuss momentarily:\n",
    "\n",
    "```python\n",
    "# Signature in Pandas v0.18\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "```\n",
    "\n",
    "``pd.concat()`` can be used for a simple concatenation of ``Series`` or ``DataFrame`` objects, just as ``np.concatenate()`` can be used for simple concatenations of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works to concatenate higher-dimensional objects, such as ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the concatenation takes place row-wise within the ``DataFrame`` (i.e., ``axis=0``).\n",
    "Like ``np.concatenate``, ``pd.concat`` allows specification of an axis along which concatenation will take place.\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "display('df3', 'df4', \"pd.concat([df3, df4], axis=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate indices\n",
    "\n",
    "One important thing to know for ``pd.concat`` is that Pandas concatenation *preserves indices*, even if the result will have duplicate indices!\n",
    "Consider this simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = make_df('AB', [0, 1])\n",
    "y = make_df('AB', [2, 3])\n",
    "y.index = x.index  # make duplicate indices!\n",
    "display('x', 'y', 'pd.concat([x, y])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the repeated indices in the result.\n",
    "While this is techinically valid within ``DataFrame``s, the outcome is often undesirable. Meaning avoid this unless you want a headache. \n",
    "``pd.concat()`` gives us a few ways to handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catching the repeats as an error\n",
    "\n",
    "If you'd like to simply verify that the indices in the result of ``pd.concat()`` do not overlap, you can specify the ``verify_integrity`` flag.\n",
    "With this set to True, the concatenation will raise an exception if there are duplicate indices.\n",
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will throw an error\n",
    "pd.concat([x, y], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignoring the index\n",
    "\n",
    "Sometimes the index itself does not matter, and you would prefer it to simply be ignored.\n",
    "This option can be specified using the ``ignore_index`` flag.\n",
    "With this set to true, the concatenation will create a new integer index for the resulting ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('x', 'y', 'pd.concat([x, y], ignore_index=True)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding MultiIndex keys\n",
    "\n",
    "Another option is to use the ``keys`` option to specify a label for the data sources; the result will be a hierarchically indexed series containing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('x', 'y', \"pd.concat([x, y], keys=['x', 'y'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation with joins\n",
    "\n",
    "In the simple examples we just looked at, we were mainly concatenating ``DataFrame``s with shared column names.\n",
    "In practice, data from different sources might have different sets of column names, and ``pd.concat`` offers several options in this case.\n",
    "Consider the concatenation of the following two ``DataFrame``s, which have some (but not all!) columns in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the entries for which no data is available are filled with NA values.\n",
    "To change this, we can specify one of several options for the ``join`` and ``join_axes`` parameters of the concatenate function.\n",
    "By default, the join is a union of the input columns (``join='outer'``), but we can change this to an intersection of the columns using ``join='inner'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join='inner')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of options of the ``pd.concat`` function allows a wide range of possible behaviors when joining two datasets; keep these in mind as you use these tools for your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll look at another more powerful approach to combining data from multiple sources, the database-style merges/joins implemented in ``pd.merge``.\n",
    "For more information on ``concat()``, ``append()``, and related functionality, see the [\"Merge, Join, and Concatenate\" section](http://pandas.pydata.org/pandas-docs/stable/merging.html) of the Pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets: Merge and Join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations.\n",
    "If you have ever worked with databases, you should be familiar with this type of data interaction.\n",
    "The main interface for this is the ``pd.merge`` function, and we'll see few examples of how this can work in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Algebra\n",
    "\n",
    "The behavior implemented in ``pd.merge()`` is a subset of what is known as *relational algebra*, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases.\n",
    "The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset.\n",
    "With this lexicon of fundamental operations implemented efficiently in a database or other program, a wide range of fairly complicated composite operations can be performed.\n",
    "\n",
    "Pandas implements several of these fundamental building-blocks in the ``pd.merge()`` function and the related ``join()`` method of ``Series`` and ``Dataframe``s.\n",
    "As we will see, these let you efficiently link data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of Joins\n",
    "\n",
    "The ``pd.merge()`` function implements a number of types of joins: the *one-to-one*, *many-to-one*, and *many-to-many* joins.\n",
    "All three types of joins are accessed via an identical call to the ``pd.merge()`` interface; the type of join performed depends on the form of the input data.\n",
    "Here we will show simple examples of the three types of merges, and discuss detailed options further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one joins\n",
    "\n",
    "Perhaps the simplest type of merge expresion is the one-to-one join, which is in many ways very similar to the column-wise concatenation seen in [Combining Datasets: Concat & Append](03.06-Concat-And-Append.ipynb).\n",
    "As a concrete example, consider the following two ``DataFrames`` which contain information on several employees in a company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "display('df1', 'df2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine this information into a single ``DataFrame``, we can use the ``pd.merge()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``pd.merge()`` function recognizes that each ``DataFrame`` has an \"employee\" column, and automatically joins using this column as a key.\n",
    "The result of the merge is a new ``DataFrame`` that combines the information from the two inputs.\n",
    "Notice that the order of entries in each column is not necessarily maintained: in this case, the order of the \"employee\" column differs between ``df1`` and ``df2``, and the ``pd.merge()`` function correctly accounts for this.\n",
    "Additionally, keep in mind that the merge in general discards the index, except in the special case of merges by index (see the ``left_index`` and ``right_index`` keywords, discussed momentarily)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-one joins are joins in which one of the two key columns contains duplicate entries.\n",
    "For the many-to-one case, the resulting ``DataFrame`` will preserve those duplicate entries as appropriate.\n",
    "Consider the following example of a many-to-one join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting ``DataFrame`` has an aditional column with the \"supervisor\" information, where the information is repeated in one or more locations as required by the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-many joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined.\n",
    "If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge.\n",
    "This will be perhaps most clear with a concrete example.\n",
    "Consider the following, where we have a ``DataFrame`` showing one or more skills associated with a particular group.\n",
    "By performing a many-to-many join, we can recover the skills associated with any individual person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display('df1', 'df5', \"pd.merge(df1, df5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three types of joins can be used with other Pandas tools to implement a wide array of functionality.\n",
    "But in practice, datasets are rarely as clean as the one we're working with here.\n",
    "In the following section we'll consider some of the options provided by ``pd.merge()`` that enable you to tune how the join operations work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the Merge Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen the default behavior of ``pd.merge()``: it looks for one or more matching column names between the two inputs, and uses this as the key.\n",
    "However, often the column names will not match so nicely, and ``pd.merge()`` provides a variety of options for handling this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``on`` keyword\n",
    "\n",
    "Most simply, you can explicitly specify the name of the key column using the ``on`` keyword, which takes a column name or a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option works only if both the left and right ``DataFrame``s have the specified column name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``left_on`` and ``right_on`` keywords\n",
    "\n",
    "At times you may wish to merge two datasets with different column names; for example, we may have a dataset in which the employee name is labeled as \"name\" rather than \"employee\".\n",
    "In this case, we can use the ``left_on`` and ``right_on`` keywords to specify the two column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "display('df1', 'df3', 'pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has a redundant column that we can drop if desired–for example, by using the ``drop()`` method of ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ``left_index`` and ``right_index`` keywords\n",
    "\n",
    "Sometimes, rather than merging on a column, you would instead like to merge on an index.\n",
    "For example, your data might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df2a = df2.set_index('employee')\n",
    "display('df1a', 'df2a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the index as the key for merging by specifying the ``left_index`` and/or ``right_index`` flags in ``pd.merge()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df2a',\n",
    "        \"pd.merge(df1a, df2a, left_index=True, right_index=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, ``DataFrame``s implement the ``join()`` method, which performs a merge that defaults to joining on indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df2a', 'df1a.join(df2a)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to mix indices and columns, you can combine ``left_index`` with ``right_on`` or ``left_on`` with ``right_index`` to get the desired behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df3', \"pd.merge(df1a, df3, left_index=True, right_on='name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these options also work with multiple indices and/or multiple columns; the interface for this behavior is very intuitive.\n",
    "For more information on this, see the [\"Merge, Join, and Concatenate\" section](http://pandas.pydata.org/pandas-docs/stable/merging.html) of the Pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Set Arithmetic for Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join.\n",
    "This comes up when a value appears in one key column but not the other. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "display('df6', 'df7', 'pd.merge(df6, df7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have merged two datasets that have only a single \"name\" entry in common: Mary.\n",
    "By default, the result contains the *intersection* of the two sets of inputs; this is what is known as an *inner join*.\n",
    "We can specify this explicitly using the ``how`` keyword, which defaults to ``\"inner\"``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options for the ``how`` keyword are ``'outer'``, ``'left'``, and ``'right'``.\n",
    "An *outer join* returns a join over the union of the input columns, and fills in all missing values with NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='outer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *left join* and *right join* return joins over the left entries and right entries, respectively.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='left')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output rows now correspond to the entries in the left input. Using\n",
    "``how='right'`` works in a similar manner.\n",
    "\n",
    "All of these options can be applied straightforwardly to any of the preceding join types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping Column Names: The ``suffixes`` Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may end up in a case where your two input ``DataFrame``s have conflicting column names.\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [1, 2, 3, 4]})\n",
    "df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [3, 1, 4, 2]})\n",
    "display('df8', 'df9', 'pd.merge(df8, df9, on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the output would have two conflicting column names, the merge function automatically appends a suffix ``_x`` or ``_y`` to make the output columns unique.\n",
    "If these defaults are inappropriate, it is possible to specify a custom suffix using the ``suffixes`` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display('df8', 'df9', 'pd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These suffixes work in any of the possible join patterns, and work also if there are multiple overlapping columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on these patterns, see [Aggregation and Grouping](03.08-Aggregation-and-Grouping.ipynb) where we dive a bit deeper into relational algebra.\n",
    "Also see the [Pandas \"Merge, Join and Concatenate\" documentation](http://pandas.pydata.org/pandas-docs/stable/merging.html) for further discussion of these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: US States Data\n",
    "\n",
    "## Note this is real world data with messy bits.  I'm going to go through and show some tricky bits.\n",
    "\n",
    "\n",
    "Merge and join operations come up most often when combining data from different sources.\n",
    "Here we will consider an example of some data about US states and their populations.\n",
    "The data files can be found at http://github.com/jakevdp/data-USstates/:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the three datasets, using the Pandas ``read_csv()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop = pd.read_csv('state-population.csv')\n",
    "areas = pd.read_csv('state-areas.csv')\n",
    "abbrevs = pd.read_csv('state-abbrevs.csv')\n",
    "\n",
    "display('pop.head()', 'areas.head()', 'abbrevs.head()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this information, say we want to compute a relatively straightforward result: rank US states and territories by their 2010 population density.\n",
    "We clearly have the data here to find this result, but we'll have to combine the datasets to find the result.\n",
    "\n",
    "We'll start with a many-to-one merge that will give us the full state name within the population ``DataFrame``.\n",
    "We want to merge based on the ``state/region``  column of ``pop``, and the ``abbreviation`` column of ``abbrevs``.\n",
    "We'll use ``how='outer'`` to make sure no data is thrown away due to mismatched labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(pop, abbrevs, how='outer',\n",
    "                  left_on='state/region', right_on='abbreviation')\n",
    "merged = merged.drop('abbreviation', axis=1) # drop duplicate info\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check whether there were any mismatches here, which we can do by looking for rows with nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the ``population`` info is null; let's figure out which these are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged[merged['population'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that all the null population values are from Puerto Rico prior to the year 2000; this is likely due to this data not being available from the original source.\n",
    "\n",
    "More importantly, we see also that some of the new ``state`` entries are also null, which means that there was no corresponding entry in the ``abbrevs`` key!\n",
    "Let's figure out which regions lack this match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.loc[merged['state'].isnull(), 'state/region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly infer the issue: our population data includes entries for Puerto Rico (PR) and the United States as a whole (USA), while these entries do not appear in the state abbreviation key.\n",
    "We can fix these quickly by filling in appropriate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'\n",
    "merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'\n",
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more nulls in the ``state`` column: we're all set!\n",
    "\n",
    "Now we can merge the result with the area data using a similar procedure.\n",
    "Examining our results, we will want to join on the ``state`` column in both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = pd.merge(merged, areas, on='state', how='left')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's check for nulls to see if there were any mismatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are nulls in the ``area`` column; we can take a look to see which regions were ignored here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final['state'][final['area (sq. mi)'].isnull()].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our ``areas`` ``DataFrame`` does not contain the area of the United States as a whole.\n",
    "We could insert the appropriate value (using the sum of all state areas, for instance), but in this case we'll just drop the null values because the population density of the entire United States is not relevant to our current discussion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final.dropna(inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data we need. To answer the question of interest, let's first select the portion of the data corresponding with the year 2000, and the total population.\n",
    "We'll use the ``query()`` function to do this quickly (this requires the ``numexpr`` package to be installed; see [High-Performance Pandas: ``eval()`` and ``query()``](03.12-Performance-Eval-and-Query.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2010 = final.query(\"year == 2010 & ages == 'total'\")\n",
    "data2010.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the population density and display it in order.\n",
    "We'll start by re-indexing our data on the state, and then compute the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2010.set_index('state', inplace=True)\n",
    "density = data2010['population'] / data2010['area (sq. mi)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "density.sort_values(ascending=False, inplace=True)\n",
    "density.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a ranking of US states plus Washington, DC, and Puerto Rico in order of their 2010 population density, in residents per square mile.\n",
    "We can see that by far the densest region in this dataset is Washington, DC (i.e., the District of Columbia); among states, the densest is New Jersey.\n",
    "\n",
    "We can also check the end of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "density.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the least dense state, by far, is Alaska, averaging slightly over one resident per square mile.\n",
    "\n",
    "This type of messy data merging is a common task when trying to answer questions using real-world data sources.\n",
    "I hope that this example has given you an idea of the ways you can combine tools we've covered in order to gain insight from your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excerise\n",
    "\n",
    "This excercise will bring together skills from the last 3 weeks. \n",
    "\n",
    "In some of the previous weeks we have been looking at babyname popularity.   One problem with the way we were calculating popularity is that it was confounded by population growth.  That is more babies were born overall so an increase in the number of babies named e.g. \"Michael\" could just reflect population growth, and not that the name is actually growing in popularity.  Another way to quantify popularity is to use percentange.  That is, what percentage of all babies have a given name.  The percentage measure normalizes for the total number of babies born.   The excercise this week is to calculate the percentage of all births that each name accounts for and use this quantity to look at several things. \n",
    "\n",
    "\n",
    "## Calculate percentage\n",
    "\n",
    "You can do this in a few steps.  \n",
    "\n",
    "1) Create a new dataframe that has the total births for a given year. \n",
    "\n",
    "2) Join this new dataframe with the original dataframe \n",
    "\n",
    "3) Calculate the percentage and put it into a new column\n",
    "\n",
    "## Use the percentage\n",
    "\n",
    "What happened to the popularity of the name Mary? Use a plot to look at the change in popularity over the years for both the count and the percentage. Is there a difference between the two ways to quantify popularity?\n",
    "\n",
    "There are also interesting social/cultural patterns.  What does the popularity graph for the name \"Hillary\" look like? \"Zayn\"? Can you suggest other names to try?\n",
    "\n",
    "# Compare across states.  \n",
    "\n",
    "The variables `babynames_ca_df` contains names from California, and `babynames_ny_df` contains names from New York. Use a join\n",
    "\n",
    "## Stretch goal\n",
    "How much more diverse have names gotten over the century? Lot's of ways to quantify this.  We had looked at the number of unique names before.  Lets \n",
    "now try looking at the percentage of babies given the most popular name.  \n",
    "\n",
    "\n",
    "## Stretch goal 2\n",
    "An interesting question could be: In what year would a child be most likely to share the same name as a classmate?  This is an example of a simple question to state with actually a difficult answer. You can't just count the numbe of unique names.  For example you could have 2 names Alice and Bob and 50% Alice and 50% Bob so there's a 50/50 chance to share a name.   Next you could have 3 names, Alice, Bob, and Taylor. You might think more names so fewer overlaps.  However if 98% were named Taylor, and 1% Alice and 1% Bob there would actually be a higher likelihood to share a name compared with the 2 name case.  \n",
    "\n",
    "A more \"proper\" way to answer this question would be to either do a \"monte carlo\" simulation by drawing names at random and counting how many duplicates. Doing this can quickly take a long time because you need to do millions of draws.  But it is useful because you don't have to make many assumptions to get to an answer.  \n",
    "\n",
    "Another way is to  calculate the \"entropy\" of the name distribtion.  Entropy is a measure of the randomness of a distribution. That's a bit beyond the scope of this module   Low entropy means you are likely to get the same thing repeated, higher entropy means it's less likely.   You can use the scipy.stats.entropy function.  We haven't done this much, but you can use any functon that turns a Series into a single number to aggregate after a groupby().   Do note when you pass in a function this way you give the name of the function WITHOUT quotes \"\" or parenthesis ()\n",
    "\n",
    "`function_name`\n",
    "\n",
    " **NOT** `function_name()` \n",
    "\n",
    " **NOT** `\"function_name\"`\n",
    "\n",
    " **NOT** `\"function_name()\"`\n",
    "\n",
    " e.g.\n",
    "\n",
    " ```python\n",
    "\n",
    " custom_aggregated_df = df.groupby('grouping_column').agg( { 'column_to_calculate':function_name })\n",
    "```\n",
    " \n",
    "\n",
    "So how about just calculating a simple metric that relates to the popularity of names?\n",
    "\n",
    "How about the average of the percentage of names?\n",
    "If the average is high a lot of babies share a name.  If the average is low lots of different babies. \n",
    "\n",
    "How about the peak percentage?  \n",
    "If the peak is high there is a name that is very common.\n",
    "\n",
    "Finaly, if you're feeling like trying, try using the entropy function.   \n",
    "\n",
    "How do these methods compare for addressing the question?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby usage help.\n",
    "#Sometimes you'll want to apply different functions to different columns of a groupby object.\n",
    "#Here is an example of how to do that.\n",
    "#It also allows you to specify just a few columns you want to analyze\n",
    "\n",
    "\n",
    "#Here is a custom\n",
    "def my_fuction (input):\n",
    "    output = np.mean(input)\n",
    "    return output\n",
    "\n",
    "#Here is a dictionary of functions that can be applied to a groupby structure   \n",
    "#You use this dictionary to specify which functions to apply to which columns\n",
    "#For each column you can also give it a list of functions to apply to that column if you want more than 1 thing calculated\n",
    "how_to_aggregate_dictionary = {\n",
    "    \"Column_name1\": \"sum\",  #<-- This is a string of a built in aggregate function.  This is a single thing to calculate for column 1\n",
    "    \"Column_name2\": [np.mean,'sum'], #<-- This does 2 things a numpy function that will be applied to the field and a string of a builtin aggregate function\n",
    "    \"Column_name3\": my_function, #<-- This is a custom function that will be applied to the field\n",
    "}\n",
    "\n",
    "#Often you'll see things in a more compact form like this:\n",
    "#Note the strung \"sum\" is a built in aggregate functino that you can use.  \n",
    "#You can also use numpy functions like np.median   NOTE for this you do note use \"\" to pass a string\n",
    "# You do NOT use () to execute the function.   You just pass the function object by just giving the function name without the ()\n",
    "examply_short_dict = {\"Fieldname\": \"sum\", \"Fieldname2\": np.median}\n",
    "\n",
    "\n",
    "df.groupby('Field').agg( how_to_aggregate_dictionary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard import and file loading setup for the babynames dataset. \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "import os.path\n",
    "import zipfile\n",
    "\n",
    "data_url = \"https://www.ssa.gov/oact/babynames/state/namesbystate.zip\"\n",
    "#Loading it from week2 folder\n",
    "local_filename = \"../week_2/babynamesbystate.zip\"\n",
    "if not os.path.exists(local_filename): # if the data exists don't download again\n",
    "    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n",
    "        f.write(resp.read())\n",
    "\n",
    "zf = zipfile.ZipFile(local_filename, 'r')\n",
    "\n",
    "\n",
    "ny_name = 'NY.TXT'\n",
    "field_names = ['State', 'Sex', 'Year', 'Name', 'Count']\n",
    "with zf.open(ny_name) as fh:\n",
    "    babynames = pd.read_csv(fh, header=None, names=field_names)\n",
    "\n",
    "babynames_ny_df = babynames\n",
    "\n",
    "ca_name = 'CA.TXT'\n",
    "\n",
    "with zf.open(ca_name) as fh:\n",
    "    babynames = pd.read_csv(fh, header=None, names=field_names)\n",
    "\n",
    "babynames_ca_df = babynames\n",
    "\n",
    "babynames.sample(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create a new dataframe that has the total births for a given year. \n",
    "#Hint: use groupby and agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Join this new dataframe with the original dataframe \n",
    "# hint: we want the combined dataframe to be the same size as the original. \n",
    "# so the total births are available for easily computing the next step\n",
    "# The key is to pick the correct columns you want to use to link theses two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Calculate the percentage and put it into a new column\n",
    "# percentage formula is: 100 * births / total births"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the percentage\n",
    "\n",
    "#What happened to the popularity of the name Mary? Use a plot to look at the change\n",
    "#in popularity over the years for both the count and the percentage. \n",
    "#Is there a difference between the two ways to quantify popularity?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#There are also interesting social/cultural patterns.  \n",
    "#What does the popularity graph for the name \"Hillary\" look like? \n",
    "#  Can you suggest other names to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How much more diverse have names gotten over the century? \n",
    "#Lot's of ways to quantify this.  We had looked at the number of unique names before. \n",
    "#  Lets now try looking at the percentage of babies given the most popular name.\n",
    "#  e.g. do 10% of people give babies the most popular name?   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stretch Goals 2\n",
    "\n",
    "#Average Percentage in a year\n",
    "\n",
    "\n",
    "#Peak percentage in a year\n",
    "\n",
    "\n",
    "#Entropy\n",
    "#Entropy requires importing the scipy library.\n",
    "import ...\n",
    "\n",
    "\n",
    "\n",
    "#How will you compare these measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jmaPy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
